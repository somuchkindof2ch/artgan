{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn import preprocessing\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get link to directory\n",
    "path_to_train_info = os.path.join(os.getenv('DATASETS'), 'ArtRecognision\\\\train_info.csv')\n",
    "path_to_train_set = os.path.join(os.getenv('DATASETS'), 'ArtRecognision\\\\train_2\\\\')\n",
    "path_to_transformed_train_set = os.path.join(os.getenv('DATASETS'), 'ArtRecognision\\\\transformed_images\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>style</th>\n",
       "      <th>genre</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102257.jpg</td>\n",
       "      <td>5b39c876740bfc1cfaf544721c43cac3</td>\n",
       "      <td>Uriel</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>abstract</td>\n",
       "      <td>1955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75232.jpg</td>\n",
       "      <td>5b39c876740bfc1cfaf544721c43cac3</td>\n",
       "      <td>Vir Heroicus Sublimis</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>abstract</td>\n",
       "      <td>1950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29855.jpg</td>\n",
       "      <td>96e5bc98488ed589b9bf17ad9fd09371</td>\n",
       "      <td>Night March of a Hundred Demons (left half)</td>\n",
       "      <td>Yamato-e</td>\n",
       "      <td>mythological painting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62252.jpg</td>\n",
       "      <td>5b39c876740bfc1cfaf544721c43cac3</td>\n",
       "      <td>Who’s Afraid of Red,  Yellow and Blue II</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>abstract</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63861.jpg</td>\n",
       "      <td>5b39c876740bfc1cfaf544721c43cac3</td>\n",
       "      <td>Black Fire I</td>\n",
       "      <td>Color Field Painting</td>\n",
       "      <td>abstract</td>\n",
       "      <td>1963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79428</th>\n",
       "      <td>23862.jpg</td>\n",
       "      <td>67959e4e5df05b3d9db7c97fd9a0b0f6</td>\n",
       "      <td>Number 547</td>\n",
       "      <td>Abstract Expressionism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1954.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79429</th>\n",
       "      <td>25525.jpg</td>\n",
       "      <td>2d72f2000c42051e7c350a39bdce9bc1</td>\n",
       "      <td>Number 13A (Arabesque)</td>\n",
       "      <td>Action painting</td>\n",
       "      <td>abstract</td>\n",
       "      <td>1948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79430</th>\n",
       "      <td>47038.jpg</td>\n",
       "      <td>f920b951670e3d8dc3c759f12ced7a3e</td>\n",
       "      <td>St. Francis of Assisi</td>\n",
       "      <td>Baroque</td>\n",
       "      <td>religious painting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79431</th>\n",
       "      <td>9021.jpg</td>\n",
       "      <td>e4183fd3d19c2bca8b7c56d19af92252</td>\n",
       "      <td>Thebe's Revenge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>marina</td>\n",
       "      <td>1982.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79432</th>\n",
       "      <td>36564.jpg</td>\n",
       "      <td>0c491bebb405572a8fca52e950a45c20</td>\n",
       "      <td>House Manhufe</td>\n",
       "      <td>Cubism</td>\n",
       "      <td>landscape</td>\n",
       "      <td>1913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79433 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         filename                            artist  \\\n",
       "0      102257.jpg  5b39c876740bfc1cfaf544721c43cac3   \n",
       "1       75232.jpg  5b39c876740bfc1cfaf544721c43cac3   \n",
       "2       29855.jpg  96e5bc98488ed589b9bf17ad9fd09371   \n",
       "3       62252.jpg  5b39c876740bfc1cfaf544721c43cac3   \n",
       "4       63861.jpg  5b39c876740bfc1cfaf544721c43cac3   \n",
       "...           ...                               ...   \n",
       "79428   23862.jpg  67959e4e5df05b3d9db7c97fd9a0b0f6   \n",
       "79429   25525.jpg  2d72f2000c42051e7c350a39bdce9bc1   \n",
       "79430   47038.jpg  f920b951670e3d8dc3c759f12ced7a3e   \n",
       "79431    9021.jpg  e4183fd3d19c2bca8b7c56d19af92252   \n",
       "79432   36564.jpg  0c491bebb405572a8fca52e950a45c20   \n",
       "\n",
       "                                             title                   style  \\\n",
       "0                                            Uriel    Color Field Painting   \n",
       "1                            Vir Heroicus Sublimis    Color Field Painting   \n",
       "2      Night March of a Hundred Demons (left half)                Yamato-e   \n",
       "3         Who’s Afraid of Red,  Yellow and Blue II    Color Field Painting   \n",
       "4                                    Black Fire I     Color Field Painting   \n",
       "...                                            ...                     ...   \n",
       "79428                                   Number 547  Abstract Expressionism   \n",
       "79429                       Number 13A (Arabesque)         Action painting   \n",
       "79430                        St. Francis of Assisi                 Baroque   \n",
       "79431                              Thebe's Revenge                     NaN   \n",
       "79432                               House Manhufe                   Cubism   \n",
       "\n",
       "                       genre    date  \n",
       "0                   abstract  1955.0  \n",
       "1                   abstract  1950.0  \n",
       "2      mythological painting     NaN  \n",
       "3                   abstract     NaN  \n",
       "4                   abstract  1963.0  \n",
       "...                      ...     ...  \n",
       "79428                    NaN  1954.0  \n",
       "79429               abstract  1948.0  \n",
       "79430     religious painting     NaN  \n",
       "79431                 marina  1982.0  \n",
       "79432              landscape    1913  \n",
       "\n",
       "[79433 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load datasets\n",
    "train_set = pd.read_csv(path_to_train_info)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of images\n",
    "onlyfiles = [f for f in listdir(path_to_train_set) if isfile(join(path_to_train_set, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave data only for the downloaded images\n",
    "filtered_train_set = train_set[train_set.filename.isin(onlyfiles)]\n",
    "final_train_dataset = filtered_train_set[['filename', 'style', 'genre']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowest_size(name_list, directory_path):\n",
    "   # name_list = img_column.tolist()\n",
    "    first_img = cv2.imread(os.path.join(directory_path, str(name_list[0])))\n",
    "    min_value = min(first_img.shape[:-1])\n",
    "    for name in name_list[1:]:\n",
    "        img=cv2.imread(os.path.join(directory_path, name))\n",
    "        if min(img.shape[:-1]) < min_value:\n",
    "            min_value = min(img.shape[:-1])\n",
    "\n",
    "    return min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get dict where keys are image filenames, values - image minimum size and minimum size to maximum size ratio\n",
    "def get_min_and_ratio_dict(img_column, directory_path):\n",
    "    name_list = img_column.tolist()\n",
    "    min_dict = {}\n",
    "    for name in name_list:\n",
    "        img=cv2.imread(os.path.join(directory_path, name)) \n",
    "        min_dict[name] = min(img.shape[:-1]), min(img.shape[:-1])/max(img.shape[:-1])\n",
    "    return min_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction to crop images with square\n",
    "def crop_image(image, crop_size):\n",
    "    height, width, _ = image.shape\n",
    "    cropped_img = image[(height-crop_size)//2 : (height+crop_size)//2, (width-crop_size)//2 : (width+crop_size)//2].copy()\n",
    "    return cropped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(file_list, path_to_train_set, path_to_transformed_train_set, crop_size, img_size):\n",
    "    for name in final_train_dataset['filename']:\n",
    "        try:\n",
    "            img=cv2.imread(os.path.join(path_to_train_set, name))\n",
    "            c_image = crop_image(img, crop_size)\n",
    "            r_image=cv2.resize(c_image,(img_size,img_size),interpolation=cv2.INTER_AREA)\n",
    "            t_image = torch.Tensor(r_image)\n",
    "            new_name = name.split(\".\", 1)[0] + \".pt\"\n",
    "            torch.save(t_image, path_to_transformed_train_set + new_name)\n",
    "\n",
    "        except:\n",
    "            continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_min_and_ratio_dict = get_min_and_ratio_dict(final_train_dataset['filename'], path_to_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ratio histogram\n",
    "plt.hist([d[1] for d in image_min_and_ratio_dict.values()], bins=750)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#min size histogram\n",
    "plt.hist([d[0] for d in image_min_and_ratio_dict.values()], bins=750)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean and std of the ratio of images\n",
    "image_ratio_standart_deviation = statistics.pstdev([d[1] for d in image_min_and_ratio_dict.values()]) \n",
    "image_ratio_mean = statistics.mean([d[1] for d in image_min_and_ratio_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11438913251732333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ratio_standart_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7435620707364642"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ratio_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean and std of the minimum sizes of images\n",
    "min_size_standart_deviation = statistics.pstdev([d[0] for d in image_min_and_ratio_dict.values()]) \n",
    "min_size_mean = statistics.mean([d[0] for d in image_min_and_ratio_dict.values()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559.2099676383365"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_size_standart_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855.2864176774349"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_size_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7797"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leave only those whose image ratio lay to the right of the lower border\n",
    "filtered_dict = {key:val for key, val in image_min_and_ratio_dict.items() if val[1] >= (image_ratio_mean - 1.5*image_ratio_standart_deviation)}\n",
    "len(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6821"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leave only those whose minimum size lay in the borders of 2 standart deviations (~95%)\n",
    "cleaned_dict = {key:val for key, val in filtered_dict.items() if val[0] >= (min_size_mean - min_size_standart_deviation) and val[0] <= (min_size_mean + min_size_standart_deviation)}\n",
    "len(cleaned_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort our cleaned dictionary by values\n",
    "sorted_dict = {k: v for k, v in sorted(cleaned_dict.items(), key=lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because we have sorted dictionary, we know the minimum size of each image, \n",
    "#so at each iteration we take keys of the 50 elements (in other words - filenames), \n",
    "#load these images, crop them by minimum element of subset (in our case subset size is 50), then resize to 224, turn to tensor and save  \n",
    "for i in range(0, len(sorted_dict), 50): \n",
    "    dict_slice = list(sorted_dict.keys())[i:i+50]\n",
    "    crop_size = lowest_size(dict_slice, path_to_train_set)\n",
    "    transform_image(dict_slice, path_to_train_set, path_to_transformed_train_set, crop_size, 224)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.fc = nn.Sequential(nn.Linear(1000, 512),\n",
    "                                 nn.Tanh(),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.Linear(512, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        n_features = 100\n",
    "\n",
    "        def block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "            layers = [nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding)]\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append([nn.ConvTranspose2d(in_channels, out_channels, 1, 1, 1)])\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append([nn.ConvTranspose2d(in_channels, out_channels, 1, 1, 1)])\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            layers.append([nn.ConvTranspose2d(in_channels, out_channels, 1, 1, 1)])\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers     \n",
    "                \n",
    "        self.linear1= nn.Linear(n_features, 256)\n",
    "        self.linear2= nn.Linear(256, 6272)\n",
    "        \n",
    "        self.deconv1 = nn.Sequential(*block(128, 128, 4, 2, 1))      \n",
    "        self.deconv2 = nn.Sequential(*block(128, 128, 4, 2, 1))        \n",
    "        self.deconv3 = nn.Sequential(*block(256, 128, 4, 2, 1))\n",
    "        self.deconv4 = nn.Sequential(*block(384, 128, 4, 2, 1))\n",
    "        self.deconv5 = nn.Sequential(*block(512, 128, 4, 2, 1))\n",
    "        self.deconv6 = nn.Sequential(*block(128, 64, 1, 1, 1))\n",
    "        self.deconv7 = nn.Sequential(*block(64, 32, 1, 1, 1))\n",
    "        self.deconv8 = nn.ConvTranspose2d(32, 3, 1, 1, 1)\n",
    "        \n",
    "        \n",
    "        def forward(self, x):\n",
    "            \n",
    "            linear1 = F.LeakyReLU(self.linear1(x))\n",
    "            linear2 = F.LeakyReLU(self.linear2(linear1))\n",
    "            hidden1 = linear2.view(-1, 128*7*7)\n",
    "            deconv1 = self.deconv1(hidden1)\n",
    "            deconv2 = self.deconv2(deconv1)\n",
    "            c2_dense = F.LeakyReLU(torch.cat([deconv1, deconv2], 1))\n",
    "            deconv3 = self.deconv3(c2_dense)\n",
    "            c3_dense = F.LeakyReLU(torch.cat([deconv1, deconv2, deconv3], 1))                  \n",
    "            deconv4 = self.deconv4(c3_dense)         \n",
    "            c4_dense = F.LeakyReLU(torch.cat([deconv1, deconv2, deconv3, deconv4], 1))   \n",
    "            deconv5 = self.deconv5(c4_dense)\n",
    "            c5_dense = F.LeakyReLU(torch.cat([deconv1, deconv2, deconv3, deconv4, deconv5], 1))\n",
    "            deconv6 = self.deconv6(c5_dense)\n",
    "            deconv7 = self.deconv7(deconv6)\n",
    "            deconv8 = F.tanh(self.deconv8(deconv7))\n",
    "            \n",
    "            return deconv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "generator = Generator().to(device)\n",
    "discriminator = discriminator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.002, amsgrad=False)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.002, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = torch.load(path_to_transformed_train_set)\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_tensor, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gen_loss_avg.append(0)\n",
    "    disc_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for image_batch, _ in train_dataloader:\n",
    "        \n",
    "        # get dataset image and create real and fake labels for use in the loss\n",
    "        image_batch = image_batch.to(device)\n",
    "        label_real = torch.ones(image_batch.size(0), device=device)\n",
    "        label_fake = torch.zeros(image_batch.size(0), device=device)\n",
    "\n",
    "        # generate a batch of images from samples of the latent prior\n",
    "        latent = torch.randn(image_batch.size(0), 100, 1, 1, device=device)\n",
    "        fake_image_batch = generator(latent)\n",
    "        \n",
    "        # train discriminator to correctly classify real and fake\n",
    "        # (detach the computation graph of the generator and the discriminator,\n",
    "        # so that gradients are not backpropagated into the generator)\n",
    "        real_pred = discriminator(image_batch).squeeze()\n",
    "        fake_pred = discriminator(fake_image_batch.detach()).squeeze()\n",
    "        disc_loss = 0.5 * (\n",
    "            F.binary_cross_entropy_with_logits(real_pred, label_real) +\n",
    "            F.binary_cross_entropy_with_logits(fake_pred, label_fake))\n",
    "        \n",
    "        disc_optimizer.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "        # train generator to output an image that is classified as real\n",
    "        fake_pred = discriminator(fake_image_batch).squeeze()\n",
    "        gen_loss = F.binary_cross_entropy(fake_pred, label_real)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        gen_loss_avg[-1] += gen_loss.item()\n",
    "        disc_loss_avg[-1] += disc_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    gen_loss_avg[-1] /= num_batches\n",
    "    disc_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average loss generator vs. discrim.: %f vs. %f' %\n",
    "          (epoch+1, num_epochs, gen_loss_avg[-1], disc_loss_avg[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
